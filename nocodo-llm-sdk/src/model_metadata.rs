//! Model metadata for all supported LLM providers
//!
//! This module contains comprehensive metadata for all supported models,
//! including capabilities, pricing, and technical specifications.

use crate::{models, providers};

/// Comprehensive model metadata
#[derive(Debug, Clone)]
pub struct ModelMetadata {
    pub provider: &'static str,
    pub model_id: &'static str,
    pub name: &'static str,
    pub context_length: u32,
    pub supports_streaming: bool,
    pub supports_tool_calling: bool,
    pub supports_vision: bool,
    pub supports_reasoning: bool,
    pub input_cost_per_token: Option<f64>,
    pub output_cost_per_token: Option<f64>,
    pub default_temperature: Option<f32>,
    pub default_max_tokens: Option<u32>,
}

/// Get all supported models with their metadata
pub fn get_all_models() -> Vec<ModelMetadata> {
    vec![
        // Claude models
        ModelMetadata {
            provider: providers::ANTHROPIC,
            model_id: models::claude::SONNET_4_5_ID,
            name: models::claude::SONNET_4_5_NAME,
            context_length: 200_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000003),
            output_cost_per_token: Some(0.000015),
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        ModelMetadata {
            provider: providers::ANTHROPIC,
            model_id: models::claude::HAIKU_4_5_ID,
            name: models::claude::HAIKU_4_5_NAME,
            context_length: 200_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000001),
            output_cost_per_token: Some(0.000005),
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        ModelMetadata {
            provider: providers::ANTHROPIC,
            model_id: models::claude::OPUS_4_5_ID,
            name: models::claude::OPUS_4_5_NAME,
            context_length: 200_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000015),
            output_cost_per_token: Some(0.000075),
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        ModelMetadata {
            provider: providers::ANTHROPIC,
            model_id: models::claude::OPUS_4_1_ID,
            name: models::claude::OPUS_4_1_NAME,
            context_length: 200_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000015),
            output_cost_per_token: Some(0.000075),
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        ModelMetadata {
            provider: providers::ANTHROPIC,
            model_id: models::claude::SONNET_4_ID,
            name: models::claude::SONNET_4_NAME,
            context_length: 200_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000003),
            output_cost_per_token: Some(0.000015),
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        // OpenAI models
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_4O_ID,
            name: models::openai::GPT_4O_NAME,
            context_length: 128_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.0000025),
            output_cost_per_token: Some(0.00001),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_4O_MINI_ID,
            name: models::openai::GPT_4O_MINI_NAME,
            context_length: 128_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.00000015),
            output_cost_per_token: Some(0.0000006),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_4_TURBO_ID,
            name: models::openai::GPT_4_TURBO_NAME,
            context_length: 128_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.00001),
            output_cost_per_token: Some(0.00003),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_4_ID,
            name: models::openai::GPT_4_NAME,
            context_length: 8192,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: false,
            input_cost_per_token: Some(0.00003),
            output_cost_per_token: Some(0.00006),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_3_5_TURBO_ID,
            name: models::openai::GPT_3_5_TURBO_NAME,
            context_length: 16_385,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: false,
            input_cost_per_token: Some(0.0000005),
            output_cost_per_token: Some(0.0000015),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_5_CODEX_ID,
            name: models::openai::GPT_5_CODEX_NAME,
            context_length: 128_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: true,
            input_cost_per_token: None, // Pricing TBD
            output_cost_per_token: None,
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        ModelMetadata {
            provider: providers::OPENAI,
            model_id: models::openai::GPT_5_1_ID,
            name: models::openai::GPT_5_1_NAME,
            context_length: 128_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: true,
            input_cost_per_token: None, // Pricing TBD
            output_cost_per_token: None,
            default_temperature: Some(1.0),
            default_max_tokens: Some(8192),
        },
        // xAI/Grok models
        ModelMetadata {
            provider: providers::XAI,
            model_id: models::grok::BETA_ID,
            name: models::grok::BETA_NAME,
            context_length: 131_072,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000005),
            output_cost_per_token: Some(0.000015),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::XAI,
            model_id: models::grok::VISION_BETA_ID,
            name: models::grok::VISION_BETA_NAME,
            context_length: 32_768,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: true,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000005),
            output_cost_per_token: Some(0.000015),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::XAI,
            model_id: models::grok::CODE_FAST_1_ID,
            name: models::grok::CODE_FAST_1_NAME,
            context_length: 131_072,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: false,
            input_cost_per_token: Some(0.000005),
            output_cost_per_token: Some(0.000015),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        // Cerebras/GLM models
        ModelMetadata {
            provider: providers::CEREBRAS,
            model_id: models::glm::LLAMA_3_3_70B_ID,
            name: models::glm::LLAMA_3_3_70B_NAME,
            context_length: 8192,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: false,
            input_cost_per_token: Some(0.0000006),
            output_cost_per_token: Some(0.0000006),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
        ModelMetadata {
            provider: providers::ZEN,
            model_id: models::glm::ZAI_GLM_4_6_ID,
            name: models::glm::ZAI_GLM_4_6_NAME,
            context_length: 128_000,
            supports_streaming: true,
            supports_tool_calling: true,
            supports_vision: false,
            supports_reasoning: false,
            input_cost_per_token: Some(0.0), // Free tier available
            output_cost_per_token: Some(0.0),
            default_temperature: Some(1.0),
            default_max_tokens: Some(4096),
        },
    ]
}

/// Get metadata for a specific model by ID
pub fn get_model_metadata(model_id: &str) -> Option<ModelMetadata> {
    get_all_models().into_iter().find(|m| m.model_id == model_id)
}

/// Get all models for a specific provider
pub fn get_models_by_provider(provider: &str) -> Vec<ModelMetadata> {
    get_all_models()
        .into_iter()
        .filter(|m| m.provider == provider)
        .collect()
}
